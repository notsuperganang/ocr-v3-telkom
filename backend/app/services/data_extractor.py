# telkom_extractor.py
# Ekstraktor Telkom Contract â€” Page 1 (One Time Charge) + merge helper untuk Page 2
# -----------------------------------------------------
# Prasyarat: file schemas.py berisi kelas Pydantic yang sudah kamu kirim.

from __future__ import annotations
import re
import json
import time
from datetime import datetime
from typing import Any, Dict, List, Optional

# Import model pydantic kamu
# Import pydantic models (support both "python -m app.services.data_extractor" and direct script run)
try:  # Preferred absolute import when package root is on sys.path
    from app.models.schemas import (
        Perwakilan,
        KontakPersonPelanggan,
        InformasiPelanggan,
        JangkaWaktu,
        KontakPersonTelkom,
        LayananUtama,
        RincianLayanan,
        TataCaraPembayaran,
        TerminPayment,
        TelkomContractData,
    )
except ModuleNotFoundError:  # Fallback for direct invocation: python app/services/data_extractor.py
    import os, sys as _sys
    _ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
    if _ROOT not in _sys.path:
        _sys.path.insert(0, _ROOT)
    from app.models.schemas import (
        Perwakilan,
        KontakPersonPelanggan,
        InformasiPelanggan,
        JangkaWaktu,
        KontakPersonTelkom,
        LayananUtama,
        RincianLayanan,
        TataCaraPembayaran,
        TerminPayment,
        TelkomContractData,
    )

# -------------------- Utilities --------------------
_MONEY_TOKEN = re.compile(r"^\s*(?:Rp\.?|Rp)?\s*[\d\.\,]+[-,]*\s*$", re.I)

def _texts_from_ocr(ocr_json: Any) -> List[str]:
    """
    Normalisasi struktur OCR ke list of strings (urutan token baris/kolom).
    Kompatibel dengan PaddleOCR-style: ocr['overall_ocr_res']['rec_texts'].
    """
    if isinstance(ocr_json, dict):
        # PaddleOCR aggregated
        overall = ocr_json.get("overall_ocr_res") or {}
        if isinstance(overall, dict) and isinstance(overall.get("rec_texts"), list):
            return [str(t) for t in overall["rec_texts"]]
        # Generic variants
        if isinstance(ocr_json.get("lines"), list):
            return [str(t) for t in ocr_json["lines"]]
        if isinstance(ocr_json.get("text"), str):
            return [ln for ln in ocr_json["text"].splitlines()]
    if isinstance(ocr_json, list):
        return [str(t) for t in ocr_json]
    if isinstance(ocr_json, str):
        return ocr_json.splitlines()
    return []

def _find_eq(texts: List[str], label: str, start: int = 0) -> Optional[int]:
    """Cari index token yang sama persis (case-insensitive) dengan label."""
    tgt = label.strip().lower()
    for i in range(start, len(texts)):
        if texts[i].strip().lower() == tgt:
            return i
    return None

def _value_after(texts: List[str], label: str, start: int = 0) -> Optional[str]:
    """Ambil token setelah label tertentu (exact-match)."""
    idx = _find_eq(texts, label, start)
    if idx is not None and idx + 1 < len(texts):
        return texts[idx + 1].strip()
    return None

def _parse_rupiah_token(tok: str) -> float:
    """Konversi token rupiah 'Rp 1.234.567,89' -> 1234567.89 (float)."""
    s = re.sub(r"[^\d,\.]", "", tok)
    
    # Handle trailing dots (like "896.462.640.-" -> "896.462.640.")
    s = s.rstrip(".")
    
    if s.count(",") == 1 and s.count(".") >= 1:
        # Format ID: titik thousand, koma decimal
        s = s.replace(".", "").replace(",", ".")
    else:
        # Handle Indonesian thousand separator (dots) without decimal comma
        if s.count(".") >= 2:  # Multiple dots = thousand separators
            s = s.replace(".", "")
        else:
            s = s.replace(",", "")
    
    try:
        return float(s)
    except Exception:
        return 0.0

def _next_money(texts: List[str], start_idx: int) -> float:
    """Ambil token uang pada posisi sesudah start_idx."""
    for j in range(start_idx + 1, min(start_idx + 5, len(texts))):
        if _MONEY_TOKEN.match(texts[j]):
            return _parse_rupiah_token(texts[j])
    # fallback: token persis setelahnya
    if start_idx + 1 < len(texts) and any(ch.isdigit() for ch in texts[start_idx + 1]):
        return _parse_rupiah_token(texts[start_idx + 1])
    return 0.0

def _find_count_after_phrase(texts: List[str], phrase: str) -> int:
    """Cari angka tepat setelah sebuah frasa (exact-match token)."""
    idx = _find_eq(texts, phrase)
    if idx is not None and idx + 1 < len(texts):
        nxt = re.sub(r"[^\d]", "", texts[idx + 1])
        if nxt.isdigit():
            return int(nxt)
    return 0

def _find_count_robust(texts: List[str], service_patterns: List[str], max_distance: int = 2) -> int:
    """
    Cari angka service dengan multiple strategies dan proximity matching.
    Simplified untuk akurasi yang lebih baik dengan prioritas exact/adjacent matching.
    """
    for pattern in service_patterns:
        # Strategy 1: Exact match + immediate next token (highest priority)
        count = _find_count_after_phrase(texts, pattern)
        if count > 0:
            return count
    
    # Strategy 2: Try all patterns with fuzzy matching (only if exact failed)
    for pattern in service_patterns:
        # Enhanced fuzzy match + immediate next token only
        idx = _find_fuzzy(texts, pattern, 0.75)  # Higher threshold for reliability
        if idx is not None and idx + 1 < len(texts):
            nxt = texts[idx + 1].strip()
            # Be strict about next token - must be pure digit
            if nxt.isdigit() and 0 <= int(nxt) <= 20:
                return int(nxt)
    
    # Strategy 3: Containing match with tight proximity (last resort)
    for pattern in service_patterns:
        idx = _find_containing(texts, pattern)
        if idx is not None:
            # Look for number in very tight radius only (adjacent tokens)
            for i in range(max(0, idx - 1), min(len(texts), idx + 2)):
                if i != idx:
                    token = texts[i].strip()
                    # Only accept pure digits for high confidence
                    if token.isdigit() and 0 <= int(token) <= 20:
                        return int(token)
    
    return 0

def _slice_after_keyword(texts: List[str], keyword: str, span: int = 12) -> str:
    """Gabung beberapa token setelah kata kunci (untuk raw_text simpanan)."""
    # cari token yang mengandung keyword (case-insensitive)
    for i, t in enumerate(texts):
        if keyword.lower() in t.lower():
            return " ".join(texts[i : min(i + span, len(texts))]).strip()
    return ""

def _get_payment_section_text(texts: List[str]) -> str:
    """
    Ekstrak teks dari seksi pembayaran untuk analisis metode pembayaran.
    Prioritas: teks di sekitar header TATA CARA PEMBAYARAN, lalu fallback ke seluruh dokumen.
    """
    # Priority 1: Section 5 dengan berbagai format
    section5_patterns = [
        "5.TATACARAPEMBAYARAN",     # PT MPG format (tanpa spasi)
        "5.TATA CARA PEMBAYARAN",   # Standard format
        "5. TATA CARA PEMBAYARAN",  # Dengan spasi setelah angka
        "5.TATACARA PEMBAYARAN",    # Variasi spasi
        "5. TATACARA PEMBAYARAN",   # Kombinasi
    ]
    
    for pattern in section5_patterns:
        payment_section = _slice_after_keyword(texts, pattern, span=20)
        if payment_section:
            return payment_section
    
    # Priority 2: Header pembayaran umum dengan berbagai format
    general_patterns = [
        "TATACARAPEMBAYARAN",       # Tanpa spasi sama sekali
        "TATA CARA PEMBAYARAN",     # Standard
        "TATACARA PEMBAYARAN",      # Partial spasi
    ]
    
    for pattern in general_patterns:
        payment_section = _slice_after_keyword(texts, pattern, span=20)
        if payment_section:
            return payment_section
    
    # Fallback: header pembayaran alternatif
    payment_section = _slice_after_keyword(texts, "PEMBAYARAN", span=20)
    if payment_section:
        return payment_section
        
    payment_section = _slice_after_keyword(texts, "KETENTUAN PEMBAYARAN", span=20)
    if payment_section:
        return payment_section
    
    # Fallback terakhir: gabung seluruh teks dokumen
    return " ".join(texts)

def _normalize_payment_text(text: str) -> str:
    """
    Normalisasi teks pembayaran untuk deteksi yang konsisten.
    """
    text = text.lower().strip()
    # Hapus spasi berlebih dan normalize
    text = re.sub(r'\s+', ' ', text)
    # Standardisasi singkatan bulan
    text = text.replace('/bln', ' per bulan')
    text = text.replace('bln', ' bulan')
    return text

def _detect_payment_type(texts: List[str]) -> tuple[str, str, str]:
    """
    Deteksi metode pembayaran dari teks OCR.
    
    Returns:
        tuple: (method_type, description, confidence)
        - method_type: "one_time_charge", "recurring", atau "unknown"
        - description: Deskripsi metode pembayaran
        - confidence: "high", "medium", "low"
    """
    # Ekstrak teks dari seksi pembayaran
    payment_text = _get_payment_section_text(texts)
    normalized_text = _normalize_payment_text(payment_text)
    
    # Pattern untuk deteksi termin (prioritas tinggi - exclude dari recurring)
    termin_patterns = [
        r'\btermin[-\s]*\d+[x]*\b',                    # Termin4X, Termin-4X, Termin 4X, Termin4, Termin 4
        r'\btermin\s+(pertama|kedua|ketiga|keempat|kelima)\b',  # Termin pertama, dll
        r'\b\d+[x]*\s*termin\b',                       # 4X termin, 4 termin, 4X Termin
        r'\btermin[-\s]*(\d+)[-\s]*[kx]\b',           # Termin-4K, Termin 4X, Termin4K
    ]
    
    # Cek eksplisit "One Time Charge" untuk prioritas tinggi
    if re.search(r'\bone\s*time\s*charge\b', normalized_text, re.I):
        return "one_time_charge", "One Time Charge", "high"
    
    # Cek apakah ada pola termin
    for pattern in termin_patterns:
        if re.search(pattern, normalized_text, re.I):
            # Deteksi termin sebagai method type tersendiri
            return "termin", "Pembayaran termin terdeteksi", "high"
    
    # Pattern untuk deteksi recurring (Indonesian + English)
    recurring_patterns = [
        r'\brecurring\b',                          # Explicit "recurring"
        r'\bperbulan\b|\bper\s*bulan\b',          # "perbulan", "per bulan"
        r'\bbulanan\b',                           # "bulanan"
        r'\bsetiap\s*bulan\b',                    # "setiap bulan"
        r'\bpembayaran\s*bulanan\b',              # "pembayaran bulanan"
        r'\btagihan\s*bulanan\b',                 # "tagihan bulanan"
        r'\blangganan\s*bulanan\b',               # "langganan bulanan"
        r'\bmonthly\b',                           # "monthly"
        r'\brecurring\s*monthly\b',               # "recurring monthly"
        r'\bbilling\s*cycle\s*:\s*monthly\b',     # "billing cycle: monthly"
        r'\bper\s*/?\s*bulan\b',                  # "per/bulan"
    ]
    
    # Cari pola recurring dalam teks seksi pembayaran (confidence tinggi)
    payment_section_only = _slice_after_keyword(texts, "TATA CARA PEMBAYARAN", span=20)
    if payment_section_only:
        normalized_section = _normalize_payment_text(payment_section_only)
        for pattern in recurring_patterns:
            match = re.search(pattern, normalized_section, re.I)
            if match:
                matched_phrase = match.group(0)
                return "recurring", f"Pembayaran bulanan terdeteksi (frasa: '{matched_phrase}')", "high"
    
    # Cari pola recurring di seluruh dokumen (confidence medium)
    for pattern in recurring_patterns:
        match = re.search(pattern, normalized_text, re.I)
        if match:
            matched_phrase = match.group(0)
            return "recurring", f"Pembayaran bulanan terdeteksi (frasa: '{matched_phrase}')", "medium"
    
    # NOTE: Removed unreliable "BULANAN" header detection as it causes false positives
    # Header tables are not reliable indicators of payment method
    # Source of truth should be section 5. TATA CARA PEMBAYARAN only
    
    # Default: tidak dapat menentukan, assume one_time_charge untuk backward compatibility
    return "one_time_charge", "Metode pembayaran tidak terdeteksi", "low"

def _extract_termin_count_only(texts: List[str]) -> Optional[int]:
    """
    Extract termin count even without amount details.
    Handles formats like 'Termin4X', 'Termin 4X', '4X termin', etc.
    """
    payment_text = _get_payment_section_text(texts)
    
    # Pattern untuk termin count saja (lebih fleksibel)
    count_patterns = [
        r'termin[-\s]*(\d+)[x]*\b',        # Termin4X, Termin-4X, Termin 4X, Termin4
        r'(\d+)[x]*\s*termin\b',           # 4X termin, 4 termin, 4X Termin
        r'termin[-\s]*(\d+)[-\s]*[kx]\b',  # Termin-4K, Termin 4X, Termin4K
    ]
    
    for pattern in count_patterns:
        match = re.search(pattern, payment_text, re.I)
        if match:
            try:
                count = int(match.group(1))
                if 1 <= count <= 20:  # Reasonable range for termin count
                    return count
            except (ValueError, IndexError):
                continue
    return None

def _extract_termin_payments(texts: List[str]) -> tuple[List[TerminPayment], int, float]:
    """
    Ekstrak daftar pembayaran termin dari teks OCR.
    
    Returns:
        tuple: (termin_list, total_count, total_amount)
    """
    # Cari teks dari seksi pembayaran
    payment_text = _get_payment_section_text(texts)
    
    # Pattern untuk menangkap termin dengan berbagai format
    # Cocokkan bulan dan tahun, lalu kata kunci sebelum Rp
    termin_pattern = re.compile(
        r'Termin[-\s]*(\d+)[,\s]*yaitu\s+periode\s+(\w+\s*\d{4})\s*(?:sebesar\s*[:]*\s*)?[:]?\s*Rp\.?([\d\.,]+)',
        re.IGNORECASE
    )
    
    termin_payments = []
    total_amount = 0.0
    
    # Cari semua matches dalam teks
    matches = termin_pattern.findall(payment_text)
    
    for match in matches:
        try:
            termin_num = int(match[0])
            period = match[1].strip()
            amount_str = match[2].strip()
            
            # Bersihkan amount string dari karakter trailing
            amount_str = re.sub(r'[^\d\.,]', '', amount_str)
            # Parse amount dengan handling format Indonesia (titik sebagai thousand separator, koma sebagai decimal)
            amount = _parse_rupiah_token("Rp " + amount_str)
            
            # Buat raw text untuk debugging
            raw_match = re.search(
                rf'Termin[-\s]*{termin_num}[^R]*?[Rp\.\s]*{re.escape(amount_str)}[,\.]?',
                payment_text, re.IGNORECASE
            )
            raw_text = raw_match.group(0) if raw_match else f"Termin-{termin_num} {period} {amount_str}"
            
            termin_payment = TerminPayment(
                termin_number=termin_num,
                period=period,
                amount=amount,
                raw_text=raw_text.strip()
            )
            
            termin_payments.append(termin_payment)
            total_amount += amount
            
        except (ValueError, IndexError) as e:
            # Log error tapi lanjutkan parsing termin lainnya
            continue
    
    # Sort berdasarkan nomor termin
    termin_payments.sort(key=lambda t: t.termin_number)

    return termin_payments, len(termin_payments), total_amount

def _auto_generate_termin_payments(count: int, total_amount: float) -> List[TerminPayment]:
    """
    Auto-generate equal termin payment splits when explicit breakdown not found.

    This function creates placeholder termin payments by evenly dividing the total
    contract cost (biaya_instalasi + biaya_langganan_tahunan) across the specified
    number of termin periods.

    Args:
        count: Number of termin periods (e.g., 4 for "Termin 4X")
        total_amount: Total contract amount to split (biaya_instalasi + biaya_langganan_tahunan)

    Returns:
        List of auto-generated TerminPayment objects with equal splits.
        Returns empty list if count or total_amount is invalid.

    Example:
        >>> _auto_generate_termin_payments(4, 100000000)
        [
            TerminPayment(termin_number=1, period="Termin 1", amount=25000000, ...),
            TerminPayment(termin_number=2, period="Termin 2", amount=25000000, ...),
            ...
        ]
    """
    if count <= 0 or total_amount <= 0:
        return []

    # Calculate equal split amount per termin
    amount_per_termin = total_amount / count

    # Generate termin payment entries
    return [
        TerminPayment(
            termin_number=i + 1,
            period=f"Termin {i + 1}",
            amount=amount_per_termin,
            raw_text=f"Auto-generated: dibagi rata dari total Rp {total_amount:,.0f}"
        )
        for i in range(count)
    ]

# -------------------- Robust Text Matching Utilities --------------------

def _normalize_text_for_matching(text: str) -> str:
    """
    Normalisasi teks OCR untuk pencocokan yang robust.
    Mengatasi variasi spasi, punctuation, dan OCR errors umum.
    """
    if not text:
        return ""
    
    # Lowercase dan hapus spasi berlebih
    text = re.sub(r'\s+', ' ', text.lower().strip())
    
    # Hapus punctuation umum yang sering muncul/hilang di OCR
    text = re.sub(r'[\.,:;\-_\(\)\[\]]+', '', text)
    
    # Normalisasi OCR errors umum
    text = text.replace('0', 'o')  # Angka nol jadi huruf O (untuk teks)
    text = text.replace('1', 'l')  # Angka satu jadi huruf l (untuk beberapa kasus)
    
    return text

def _calculate_text_similarity(text1: str, text2: str) -> float:
    """
    Hitung similarity antara dua teks menggunakan Jaccard similarity pada kata.
    Returns 0.0-1.0, dimana 1.0 = identik.
    """
    norm1 = _normalize_text_for_matching(text1)
    norm2 = _normalize_text_for_matching(text2)
    
    if not norm1 or not norm2:
        return 0.0
    
    # Jaccard similarity pada kata-kata
    words1 = set(norm1.split())
    words2 = set(norm2.split())
    
    if not words1 and not words2:
        return 1.0
    if not words1 or not words2:
        return 0.0
        
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    
    return len(intersection) / len(union) if union else 0.0

def _find_fuzzy(texts: List[str], target: str, threshold: float = 0.6, start: int = 0) -> Optional[int]:
    """
    Cari teks menggunakan fuzzy matching dengan similarity threshold.
    """
    best_match = None
    best_score = 0.0
    
    for i in range(start, len(texts)):
        score = _calculate_text_similarity(texts[i], target)
        if score >= threshold and score > best_score:
            best_score = score
            best_match = i
    
    return best_match

def _find_containing(texts: List[str], target: str, start: int = 0) -> Optional[int]:
    """
    Cari token yang mengandung target substring (case-insensitive).
    Lebih permissive daripada exact match.
    """
    target_norm = _normalize_text_for_matching(target)
    
    for i in range(start, len(texts)):
        text_norm = _normalize_text_for_matching(texts[i])
        if target_norm in text_norm or text_norm in target_norm:
            return i
    
    return None

def _find_near_label(texts: List[str], labels: List[str], start: int = 0, max_distance: int = 5) -> Optional[int]:
    """
    Cari salah satu dari beberapa variasi label dalam jarak tertentu.
    """
    for label in labels:
        # Try exact match first
        idx = _find_eq(texts, label, start)
        if idx is not None:
            return idx
            
        # Try fuzzy match
        idx = _find_fuzzy(texts, label, 0.7, start)
        if idx is not None:
            return idx
            
        # Try containing match
        idx = _find_containing(texts, label, start)
        if idx is not None:
            return idx
    
    return None

def _extract_field_multi_strategy(texts: List[str], field_patterns: List[str], start_idx: int = 0) -> Optional[str]:
    """
    Ekstrak field menggunakan multiple strategies dalam urutan prioritas.
    """
    for pattern in field_patterns:
        # Strategy 1: Exact match + next token
        idx = _find_eq(texts, pattern, start_idx)
        if idx is not None and idx + 1 < len(texts):
            candidate = texts[idx + 1].strip()
            if candidate and candidate not in ['', 'Nama', 'Alamat', 'NPWP']:  # Skip obvious labels
                return candidate
        
        # Strategy 2: Fuzzy match + next token
        idx = _find_fuzzy(texts, pattern, 0.7, start_idx)
        if idx is not None and idx + 1 < len(texts):
            candidate = texts[idx + 1].strip()
            if candidate and candidate not in ['', 'Nama', 'Alamat', 'NPWP']:
                return candidate
        
        # Strategy 3: Containing match (for concatenated text)
        idx = _find_containing(texts, pattern, start_idx)
        if idx is not None:
            text = texts[idx]
            # Try to extract value after pattern in same token
            pattern_norm = _normalize_text_for_matching(pattern)
            text_norm = _normalize_text_for_matching(text)
            pos = text_norm.find(pattern_norm)
            if pos >= 0:
                remainder = text[pos + len(pattern):].strip()
                if remainder and remainder not in ['', 'Nama', 'Alamat', 'NPWP']:
                    return remainder
    
    return None

def _extract_cost_from_biaya_section(texts: List[str], cost_type: str) -> float:
    """
    Extract cost from structured BIAYA-BIAYA section.
    Pattern: "4.BIAYA-BIAYA(Rupiah)" -> "Biaya Instalasi" -> "Rp. X" -> "Biaya Langganan Tahunan" -> "Rp. Y"
    
    Args:
        texts: List of OCR text tokens
        cost_type: "instalasi" or "langganan"
    
    Returns:
        float: Extracted cost amount or 0.0 if not found
    """
    # Find the biaya section header
    biaya_header_patterns = [
        "4.BIAYA-BIAYA(Rupiah)",
        "4. BIAYA-BIAYA(Rupiah)", 
        "4.BIAYA-BIAYA (Rupiah)",
        "4. BIAYA-BIAYA (Rupiah)",
        "BIAYA-BIAYA(Rupiah)",
        "BIAYA-BIAYA (Rupiah)"
    ]
    
    start_idx = None
    for pattern in biaya_header_patterns:
        idx = _find_eq(texts, pattern)
        if idx is not None:
            start_idx = idx
            break
    
    if start_idx is None:
        return 0.0
    
    
    # Parse structured sequence starting from biaya header
    # Expected pattern: header -> "Biaya Instalasi" -> "Rp. X" -> "Biaya Langganan Tahunan" -> "Rp. Y"
    i = start_idx + 1
    instalasi_amount = 0.0
    langganan_amount = 0.0
    
    while i < len(texts):
        current_text = texts[i].strip()
        
        # Look for instalasi cost
        if ("Biaya Instalasi" in current_text or "BiayaInstalasi" in current_text) and i + 1 < len(texts):
            next_token = texts[i + 1].strip()
            if _MONEY_TOKEN.match(next_token):
                instalasi_amount = _parse_rupiah_token(next_token)
                i += 2  # Skip the money token
                continue
        
        # Look for langganan cost with more pattern variations
        if (("Biaya Langganan Tahunan" in current_text or
             "BiayaLanggananTahunan" in current_text or
             "BiayaLangganan12Bulan" in current_text or  # PT MIFA format
             "Biaya Langganan 12 Bulan" in current_text or
             "Biaya Langganan Selama" in current_text) and i + 1 < len(texts)):
            next_token = texts[i + 1].strip()
            if _MONEY_TOKEN.match(next_token):
                langganan_amount = _parse_rupiah_token(next_token)
                i += 2  # Skip the money token
                continue
        
        # Stop if we hit the next major section
        if current_text.startswith("5.") or "TATA CARA PEMBAYARAN" in current_text:
            break
            
        i += 1
        
        # Safety limit - don't parse too far from biaya header
        if i > start_idx + 20:
            break
    
    if cost_type == "instalasi":
        return instalasi_amount
    elif cost_type == "langganan":
        return langganan_amount
    else:
        return 0.0

def _extract_cost_robust(texts: List[str], cost_patterns: List[str], max_distance: int = 10) -> float:
    """
    Ekstrak biaya dengan multiple strategies dan table parsing.
    """
    for pattern in cost_patterns:
        # Strategy 1: Exact match + next token (existing logic)
        idx = _find_eq(texts, pattern)
        if idx is not None:
            amount = _next_money(texts, idx)
            if amount > 0:
                return amount
        
        # Strategy 2: Fuzzy match + next token  
        idx = _find_fuzzy(texts, pattern, 0.7)
        if idx is not None:
            amount = _next_money(texts, idx)
            if amount > 0:
                return amount
        
        # Strategy 3: Containing match + nearby amounts
        idx = _find_containing(texts, pattern)
        if idx is not None:
            # Cari amount dalam radius max_distance tokens
            for i in range(max(0, idx - max_distance), min(len(texts), idx + max_distance + 1)):
                token = texts[i].strip()
                if _MONEY_TOKEN.match(token):
                    amount = _parse_rupiah_token(token)
                    if amount > 0:
                        return amount
        
        # Strategy 4: Look in HTML tables untuk pattern
        for text in texts:
            if '<table>' in text.lower() or '<html>' in text.lower():
                # Simple HTML parsing untuk amounts
                amounts = re.findall(r'(?:Rp\.?\s*)?(\d{1,3}(?:\.\d{3})*(?:,\d{2})?)', text)
                if amounts:
                    # Return first substantial amount (> 1000)
                    for amount_str in amounts:
                        amount = _parse_rupiah_token(amount_str)
                        if amount > 1000:  # Skip small amounts likely to be counts
                            return amount
    
    return 0.0

def _extract_cost_value_before_label(texts: List[str]) -> tuple[float, float]:
    """
    Handle cases where cost values appear BEFORE their labels in the texts array.
    This pattern appears in contracts like KONTRAK-PT-LKMS-MAHIRAH-MUAMALAH-2025.
    
    Pattern:
    "Rp. 0.," <- Biaya Instalasi value
    "Biaya Instalasi" <- Label
    "Rp. 20.113.200," <- Biaya Langganan Tahunan value  
    "Biaya Langganan Tahunan" <- Label
    
    Note: biaya_langganan_tahunan should never be free (0), only biaya_instalasi can be free.
    """
    biaya_instalasi = 0.0
    biaya_langganan_tahunan = 0.0
    
    # Look for "4. BIAYA-BIAYA" section first to locate the area
    biaya_section_idx = None
    for i, text in enumerate(texts):
        if "biaya-biaya" in text.lower() or "4. biaya-biaya" in text.lower():
            biaya_section_idx = i
            break
    
    if biaya_section_idx is not None:
        # Search in the section around "BIAYA-BIAYA" (before and after)
        search_start = max(0, biaya_section_idx - 5)
        search_end = min(len(texts), biaya_section_idx + 10)
        section_texts = texts[search_start:search_end]
        
        # Look for the pattern: amount followed by label
        instalasi_found = False
        langganan_found = False
        
        for i in range(len(section_texts) - 1):
            current_text = section_texts[i].strip()
            next_text = section_texts[i + 1].strip()
            
            # Check if current text is a money amount and next is "Biaya Instalasi"
            if (_MONEY_TOKEN.match(current_text) and 
                "biaya instalasi" in next_text.lower() and not instalasi_found):
                biaya_instalasi = _parse_rupiah_token(current_text)
                instalasi_found = True
            
            # Check if current text is a money amount and next is "Biaya Langganan Tahunan"
            elif (_MONEY_TOKEN.match(current_text) and 
                  ("biaya langganan tahunan" in next_text.lower() or "langganan tahunan" in next_text.lower()) and 
                  not langganan_found):
                amount = _parse_rupiah_token(current_text)
                # biaya_langganan_tahunan should never be 0 unless instalasi is the main cost
                if amount > 0:
                    biaya_langganan_tahunan = amount
                    langganan_found = True
    
    return biaya_instalasi, biaya_langganan_tahunan

def _extract_cost_special_cases(texts: List[str]) -> tuple[float, float]:
    """
    Handle special cases like "Free", "Rp 0", combined text patterns, and value-before-label patterns.
    """
    biaya_instalasi = 0.0
    biaya_langganan = 0.0
    
    # First check for "Free" pattern which is common and needs special handling
    full_text = " ".join(texts).lower()
    
    # PT MPG pattern: "Free", "Biaya Langganan Bulanan", "Rp896.462.640.-" as separate text blocks
    if "free" in full_text:
        if "instalasi" in full_text and "free" in full_text:
            # Instalasi free, set to 0 and find langganan amount
            biaya_instalasi = 0.0
            
            # Handle separated pattern where Free, Biaya Langganan, and amount are separate blocks
            found_free_idx = None
            found_langganan_idx = None
            amount_idx = None
            
            for i, text in enumerate(texts):
                if "free" == text.lower().strip():
                    found_free_idx = i
                elif "biaya langganan bulanan" in text.lower():
                    found_langganan_idx = i
                elif "rp" in text.lower() and re.match(r'^\s*Rp[\d\.\,\-]+', text, re.I):
                    amount_idx = i
            
            # If we found the separated pattern, extract the amount
            if found_free_idx and found_langganan_idx and amount_idx:
                # Check if the order makes sense (free, then langganan, then amount)
                if found_free_idx < found_langganan_idx < amount_idx:
                    parsed_amount = _parse_rupiah_token(texts[amount_idx])
                    biaya_langganan = parsed_amount
                    return biaya_instalasi, biaya_langganan
            
            # Try structured extraction first (more reliable)
            biaya_langganan = _extract_cost_from_biaya_section(texts, "langganan")

            # Fallback to enhanced pattern matching if structured extraction fails
            if biaya_langganan == 0.0:
                langganan_patterns = [
                    "Biaya Langganan Tahunan", "Langganan Tahunan", "Langganan Selama",
                    "Biaya Langganan Selama1Tahun", "Biaya Langganan Selama1tahun",
                    "Biaya Langganan Selama 1Tahun", "Biaya Langganan Selama 1tahun",
                    "Biaya Langganan Bulanan"  # Handle PT MPG edge case
                ]
                biaya_langganan = _extract_cost_robust(texts, langganan_patterns)
            return biaya_instalasi, biaya_langganan
        elif "langganan" in full_text and "free" in full_text:
            # Langganan free, cari instalasi amount  
            biaya_langganan = 0.0
            instalasi_patterns = ["Biaya Instalasi"]
            biaya_instalasi = _extract_cost_robust(texts, instalasi_patterns)
            return biaya_instalasi, biaya_langganan
    
    # First try structured biaya section parsing (most reliable for standard contracts)
    biaya_instalasi_structured = _extract_cost_from_biaya_section(texts, "instalasi")
    biaya_langganan_structured = _extract_cost_from_biaya_section(texts, "langganan")
    
    # Use structured results if both found or if langganan found (langganan is more critical)
    if biaya_langganan_structured > 0:
        return biaya_instalasi_structured, biaya_langganan_structured
    
    # Fallback: try the value-before-label pattern extraction
    instalasi_before, langganan_before = _extract_cost_value_before_label(texts)
    # Only use if we actually found meaningful values (not default 0,0)
    if langganan_before > 0 or (instalasi_before > 0 and langganan_before > 0):  
        return instalasi_before, langganan_before
    
    # If still no values found, try other pattern matching approaches
    
    # Case: Table-based extraction with specific patterns
    for text in texts:
        if "biaya instalasi" in text.lower() and "langganan" in text.lower():
            # Combined cost text, extract both
            amounts = re.findall(r'(?:Rp\.?\s*)?(\d{1,3}(?:\.\d{3})*(?:,\d{2})?)', text)
            if len(amounts) >= 2:
                biaya_instalasi = _parse_rupiah_token(amounts[0])
                biaya_langganan = _parse_rupiah_token(amounts[1])
                break
    
    return biaya_instalasi, biaya_langganan


# -------------------- Page 1 Extractor (One Time Charge) --------------------
def extract_from_page1_one_time(ocr_json_page1: Any) -> TelkomContractData:
    """
    Ekstraksi dari PAGE 1 untuk kasus 'One Time Charge'.
    - Mengisi: informasi_pelanggan (nama, alamat, npwp, perwakilan jika ada),
               layanan_utama (count),
               rincian_layanan (biaya instalasi & langganan tahunan),
               tata_cara_pembayaran (one_time_charge + raw_text).
    - Placeholder: kontak_person_telkom, informasi_pelanggan.kontak_person, jangka_waktu.
    """
    t0 = time.time()
    texts = _texts_from_ocr(ocr_json_page1)

    # --- Informasi Pelanggan (Enhanced Robust Extraction) ---
    nama_pelanggan = alamat = npwp = None
    perwakilan_nama = perwakilan_jabatan = None

    # Cari section pelanggan dengan multiple patterns
    # Note: Use section-specific patterns to avoid matching standalone numbers
    # The "2. PELANGGAN" pattern can match standalone "2" via containing match ("2" in "2. PELANGGAN")
    # So we need exact/fuzzy match first, then fallback to word "PELANGGAN" only
    pelanggan_patterns = ["2. PELANGGAN", "2.PELANGGAN", "2.  PELANGGAN", "2.PELANGGAN ", "PELANGGAN"]

    # Try exact match first (most reliable)
    idx_pelanggan = None
    for pattern in ["2. PELANGGAN", "2.PELANGGAN", "2.  PELANGGAN"]:
        idx = _find_eq(texts, pattern, 0)
        if idx is not None:
            idx_pelanggan = idx
            break

    # If exact failed, try fuzzy (still reliable)
    if idx_pelanggan is None:
        idx_pelanggan = _find_fuzzy(texts, "2. PELANGGAN", 0.8, 0)

    # Last resort: find "PELANGGAN" word alone
    if idx_pelanggan is None:
        idx_pelanggan = _find_eq(texts, "PELANGGAN", 0)

    if idx_pelanggan is not None:
        # Multi-strategy extraction untuk fields utama
        nama_patterns = ["Nama", "nama", "NAMA"]
        alamat_patterns = ["Alamat", "alamat", "ALAMAT", "Alamat NPWP"]  # Handle concatenated case
        npwp_patterns = ["NPWP", "npwp"]

        nama_pelanggan = _extract_field_multi_strategy(texts, nama_patterns, idx_pelanggan)
        alamat = _extract_field_multi_strategy(texts, alamat_patterns, idx_pelanggan) 
        npwp = _extract_field_multi_strategy(texts, npwp_patterns, idx_pelanggan)
        
        # Clean up NPWP if it contains unwanted text
        if npwp and "diwakili" in npwp.lower():
            npwp = None  # Reset jika dapat noise text

        # Jika alamat dan NPWP concatenated, coba pisahkan
        if alamat and not npwp:
            # Cari pola NPWP dalam alamat (XX.XXX.XXX.X-XXX.XXX)
            npwp_match = re.search(r'\d{2}\.\d{3}\.\d{3}\.\d{1}-\d{3}\.\d{3}', alamat)
            if npwp_match:
                npwp = npwp_match.group(0)
                # Hapus NPWP dari alamat
                alamat = alamat.replace(npwp, '').strip()

        # Cari perwakilan dengan flexible matching
        # Note: OCR typos include "saholeh" (missing space), "saholehï¼š" (Chinese colon)
        # Try shorter, more specific patterns first to avoid false matches via containing
        perwakilan_patterns = [
            "Diwakili secara saholeh",  # OCR typo: missing space (try first)
            "diwakili secara saholeh",  # OCR typo lowercase
            "Diwakili secara sah oleh",  # Normal format without colon
            "diwakili secara sah oleh",
        ]
        # Manual search to avoid false positives from containing match
        idx_rep = None
        for pattern in perwakilan_patterns:
            # Try exact match first
            idx = _find_eq(texts, pattern, idx_pelanggan)
            if idx is not None:
                idx_rep = idx
                break
            # Try fuzzy match
            idx = _find_fuzzy(texts, pattern, 0.7, idx_pelanggan)
            if idx is not None:
                idx_rep = idx
                break

        # If still not found, try containing match carefully (only for longer patterns)
        if idx_rep is None:
            idx_rep = _find_containing(texts, "Diwakili secara sah", idx_pelanggan)
        
        if idx_rep is not None:
            perwakilan_nama = _extract_field_multi_strategy(texts, nama_patterns, idx_rep)
            jabatan_patterns = ["Jabatan", "jabatan", "JABATAN"]
            perwakilan_jabatan = _extract_field_multi_strategy(texts, jabatan_patterns, idx_rep)

    informasi_pelanggan = InformasiPelanggan(
        nama_pelanggan=nama_pelanggan,
        alamat=alamat,
        npwp=npwp,
        perwakilan=Perwakilan(nama=perwakilan_nama, jabatan=perwakilan_jabatan) if (perwakilan_nama or perwakilan_jabatan) else None,
        kontak_person=None,  # placeholder (ada di Page 2)
    )

    # --- Layanan Utama (Enhanced Robust Counts) ---
    # Multiple patterns untuk setiap service type dengan variasi spasi OCR
    connectivity_patterns = [
        "Layanan Connectivity TELKOM",          # Standard format (SMK Penerbangan, SMKN Bireun)
        "LayananConnectivityTELKOM",            # Tanpa spasi sama sekali
        "LayananConnectivity TELKOM",           # PT MPG format (tanpa spasi setelah "Layanan")
        "Layanan ConnectivityTELKOM",           # Tanpa spasi setelah "Connectivity"
        "Connectivity TELKOM",                   # Simplified format
        "LAYANAN CONNECTIVITY TELKOM",          # Uppercase variant
        "LAYANANCONNECTIVITYTELKOM",            # Full concatenated
    ]
    
    non_connectivity_patterns = [
        "Layanan Non-Connectivity TELKOM",          # Standard format
        "LayananNon-ConnectivityTELKOM",            # Tanpa spasi setelah "Layanan"
        "Layanan Non-ConnectivityTELKOM",           # Tanpa spasi setelah "Non-Connectivity"
        "LayananNon-Connectivity TELKOM",           # Tanpa spasi sebelum, dengan spasi setelah
        "Non-Connectivity TELKOM",                   # Simplified format
        "LAYANAN NON-CONNECTIVITY TELKOM",          # Uppercase variant
        "LAYANANNON-CONNECTIVITYTELKOM",            # Full concatenated
    ]
    
    bundling_patterns = [
        "Bundling Layanan Connectivity TELKOM& Solusi",        # Standard format 
        "BundlingLayananConnectivityTELKOM&Solusi",            # PT MPG format (full concatenated)
        "Bundling LayananConnectivityTELKOM&Solusi",           # SMK Penerbangan format 
        "Bundling Layanan Connectivity TELKOM & Solusi",       # SMKN Bireun format (dengan spasi sebelum &)
        "BundlingLayanan Connectivity TELKOM&Solusi",          # Mixed format
        "Bundling LayananConnectivity TELKOM&Solusi",          # Mixed format 2
        "BUNDLING LAYANAN CONNECTIVITY TELKOM& SOLUSI",        # Uppercase variant
        "BUNDLINGLAYANANCONNECTIVITYTELKOM&SOLUSI",           # Full uppercase concatenated
    ]
    
    connectivity = _find_count_robust(texts, connectivity_patterns)
    non_connectivity = _find_count_robust(texts, non_connectivity_patterns)
    bundling = _find_count_robust(texts, bundling_patterns)
    
    layanan_utama = LayananUtama(
        connectivity_telkom=connectivity,
        non_connectivity_telkom=non_connectivity,
        bundling=bundling,
    )

    # --- Rincian Layanan (Enhanced Robust Cost Extraction) ---
    biaya_instalasi = 0.0
    biaya_langganan_tahunan = 0.0

    # Try special cases first (Free, combined patterns)
    special_instalasi, special_langganan = _extract_cost_special_cases(texts)
    if special_instalasi >= 0 or special_langganan > 0:  # Allow 0 for instalasi (Free case)
        biaya_instalasi = special_instalasi
        biaya_langganan_tahunan = special_langganan
    else:
        # Multiple patterns untuk biaya instalasi
        instalasi_patterns = [
            "Biaya Instalasi",
            "BiayaInstalasi", 
            "BIAYA INSTALASI",
            "Biaya Instalasi Rp"
        ]
        
        # Multiple patterns untuk biaya langganan (only specific patterns to avoid false matches)
        langganan_patterns = [
            "Biaya Langganan Tahunan",       # Most specific first
            "BiayaLanggananTahunan",
            "BIAYA LANGGANAN TAHUNAN", 
            "Biaya Langganan Selama1Tahun",  # Handle concatenated case
            "Biaya Langganan Selama1tahun",  # Handle lowercase OCR variation
            "Biaya Langganan Selama 1Tahun", # Handle spaced variation  
            "Biaya Langganan Selama 1tahun", # Handle spaced + lowercase variation
            "BiayaLanggananSelama1Tahun",    # Handle completely concatenated
            "BiayaLanggananSelama1tahun",    # Handle concatenated + lowercase
            "Biaya Langganan Selama",        # Keep this for general "Selama" patterns
            "Biaya Langganan Bulanan"        # Handle PT MPG edge case where contract says "Bulanan" but means annual
        ]
        
        # First try structured biaya section parsing
        biaya_langganan_tahunan = _extract_cost_from_biaya_section(texts, "langganan")
        if biaya_langganan_tahunan == 0.0:
            # Fallback to pattern-based extraction
            biaya_langganan_tahunan = _extract_cost_robust(texts, langganan_patterns)
        
        # First try structured biaya section parsing for instalasi too
        biaya_instalasi = _extract_cost_from_biaya_section(texts, "instalasi")
        if biaya_instalasi == 0.0:
            # Fallback to pattern-based extraction
            biaya_instalasi = _extract_cost_robust(texts, instalasi_patterns)

    rincian_layanan = [
        RincianLayanan(
            biaya_instalasi=biaya_instalasi,
            biaya_langganan_tahunan=biaya_langganan_tahunan,
            tata_cara_pembayaran=None,  # di level utama kita isi di bawah
        )
    ]

    # --- Tata Cara Pembayaran (Dynamic Detection) ---
    raw_tata = _slice_after_keyword(texts, "TATA CARA PEMBAYARAN", span=16)
    
    # Deteksi metode pembayaran secara dinamis
    method_type, description, confidence = _detect_payment_type(texts)
    
    # Jika termin, ekstrak detail pembayaran termin
    termin_payments = None
    total_termin_count = None
    total_amount = None
    
    if method_type == "termin":
        # Try to extract full termin details first
        termin_list, count, amount = _extract_termin_payments(texts)
        if termin_list:  # Jika berhasil ekstrak termin lengkap
            termin_payments = termin_list
            total_termin_count = count
            total_amount = amount
            description = f"Pembayaran termin ({count} periode)"
        else:
            # Fallback: extract termin count only (for edge cases without amounts)
            count_only = _extract_termin_count_only(texts)

            if count_only:
                # Calculate total contract cost for auto-generation
                total_contract_cost = biaya_langganan_tahunan + biaya_instalasi

                if total_contract_cost > 0:
                    # AUTO-GENERATE: Split total cost evenly across termins
                    termin_payments = _auto_generate_termin_payments(count_only, total_contract_cost)
                    total_termin_count = count_only
                    total_amount = total_contract_cost
                    description = f"Pembayaran termin ({count_only} periode, dibagi rata otomatis)"
                else:
                    # No cost data available, can't auto-generate amounts
                    total_termin_count = count_only
                    total_amount = 0
                    description = f"Pembayaran termin ({count_only} periode, menunggu input biaya)"
            else:
                # Last resort: termin detected but no count found
                description = "Pembayaran termin terdeteksi (gagal ekstrak detail)"
    
    tata_cara_pembayaran = TataCaraPembayaran(
        method_type=method_type,
        description=description,
        termin_payments=termin_payments,
        total_termin_count=total_termin_count,
        total_amount=total_amount,
        raw_text=raw_tata or None,
    )

    # --- Kontak Person Telkom (placeholder; ada di Page 2) ---
    kontak_person_telkom = KontakPersonTelkom(
        nama=None, jabatan=None, email=None, telepon=None
    )

    # --- Jangka Waktu (placeholder; ada di Page 2) ---
    jangka_waktu = JangkaWaktu(mulai=None, akhir=None)

    data = TelkomContractData(
        informasi_pelanggan=informasi_pelanggan,
        layanan_utama=layanan_utama,
        rincian_layanan=rincian_layanan,
        tata_cara_pembayaran=tata_cara_pembayaran,
        kontak_person_telkom=kontak_person_telkom,
        jangka_waktu=jangka_waktu,
        extraction_timestamp=datetime.now(),
        processing_time_seconds=round(time.time() - t0, 3),
    )
    return data


# -------------------- Indonesian date parsing (robust) --------------------
_ID_MONTHS = {
    "jan": 1, "januari": 1,
    "feb": 2, "februari": 2,
    "mar": 3, "maret": 3,
    "apr": 4, "april": 4,
    "mei": 5,
    "jun": 6, "juni": 6,
    "jul": 7, "juli": 7,
    "agu": 8, "agustus": 8,
    "sep": 9, "sept": 9, "september": 9,
    "okt": 10, "oktober": 10,
    "nov": 11, "november": 11,
    "des": 12, "desember": 12,
}

def _to_iso_date(y: int, m: int, d: int) -> Optional[str]:
    try:
        return f"{int(y):04d}-{int(m):02d}-{int(d):02d}"
    except Exception:
        return None

def _parse_date_id(s: str) -> Optional[str]:
    s = s.strip()
    print(f"ðŸ” DEBUG _parse_date_id: Attempting to parse '{s}'")
    
    # ISO: YYYY-MM-DD
    m = re.search(r"\b(20\d{2}|19\d{2})-(\d{1,2})-(\d{1,2})\b", s)
    if m:
        result = _to_iso_date(m.group(1), m.group(2), m.group(3))
        print(f"âœ… DEBUG _parse_date_id: ISO format matched: {result}")
        return result
    
    # DD[-/]MM[-/]YYYY
    m = re.search(r"\b(\d{1,2})[\/\-](\d{1,2})[\/\-](20\d{2}|19\d{2})\b", s)
    if m:
        d, mo, y = int(m.group(1)), int(m.group(2)), int(m.group(3))
        result = _to_iso_date(y, mo, d)
        print(f"âœ… DEBUG _parse_date_id: DD/MM/YYYY format matched: {result}")
        return result
    
    # D Month YYYY (Indonesia) with spaces
    m = re.search(r"\b(\d{1,2})\s+([A-Za-z\.]+)\s+(20\d{2}|19\d{2})\b", s)
    if m:
        d = int(m.group(1))
        mon = m.group(2).lower().strip(".")
        y = int(m.group(3))
        print(f"ðŸ” DEBUG _parse_date_id: Spaced format - day: {d}, month: '{mon}', year: {y}")
        if mon in _ID_MONTHS:
            result = _to_iso_date(y, _ID_MONTHS[mon], d)
            print(f"âœ… DEBUG _parse_date_id: Spaced format matched: {result}")
            return result
        else:
            print(f"âŒ DEBUG _parse_date_id: Month '{mon}' not found in _ID_MONTHS")
    
    # Concatenated format: DDMonthYYYY (e.g., "23Februari2027")  
    m = re.search(r"(\d{1,2})([A-Za-z]+)(\d{4})", s)  # Removed \b boundaries
    if m:
        d = int(m.group(1))
        mon = m.group(2).lower()
        y = int(m.group(3))
        print(f"ðŸ” DEBUG _parse_date_id: Concatenated format - day: {d}, month: '{mon}', year: {y}")
        if mon in _ID_MONTHS:
            result = _to_iso_date(y, _ID_MONTHS[mon], d)
            print(f"âœ… DEBUG _parse_date_id: Concatenated format matched: {result}")
            return result
        else:
            print(f"âŒ DEBUG _parse_date_id: Month '{mon}' not found in _ID_MONTHS")
    
    # Format with optional spaces: DD [space] Month YYYY (e.g., "31 Desember2025")
    m = re.search(r"(\d{1,2})\s*([A-Za-z]+)(\d{4})", s)  # Removed \b boundaries  
    if m:
        d = int(m.group(1))
        mon = m.group(2).lower()
        y = int(m.group(3))
        print(f"ðŸ” DEBUG _parse_date_id: Optional spaces format - day: {d}, month: '{mon}', year: {y}")
        if mon in _ID_MONTHS:
            result = _to_iso_date(y, _ID_MONTHS[mon], d)
            print(f"âœ… DEBUG _parse_date_id: Optional spaces format matched: {result}")
            return result
        else:
            print(f"âŒ DEBUG _parse_date_id: Month '{mon}' not found in _ID_MONTHS")
    
    # CRITICAL PATTERN: DD+Month + space + YYYY (e.g., "23Februari 2027")
    m = re.search(r"(\d{1,2})([A-Za-z]+)\s+(\d{4})", s)
    if m:
        d = int(m.group(1))
        mon = m.group(2).lower()
        y = int(m.group(3))
        print(f"ðŸ” DEBUG _parse_date_id: CRITICAL pattern - day: {d}, month: '{mon}', year: {y}")
        if mon in _ID_MONTHS:
            result = _to_iso_date(y, _ID_MONTHS[mon], d)
            print(f"âœ… DEBUG _parse_date_id: CRITICAL pattern matched: {result}")
            return result
        else:
            print(f"âŒ DEBUG _parse_date_id: Month '{mon}' not found in _ID_MONTHS")
    
    print(f"âŒ DEBUG _parse_date_id: No pattern matched for '{s}'")
    return None

# -------------------- Page 2: robust jangka waktu + kontak --------------------
def _blob(texts: List[str]) -> str:
    return "\n".join(texts)

def _norm_label(s: str) -> str:
    s = s.lower().strip()
    s = s.replace("*", "").replace(")", "").replace("(", "")
    s = s.replace(":", "")
    s = s.replace("telepon/gsm", "telepon")
    s = s.replace("e-mail", "email").replace("email", "email")
    return s.strip()  # Final strip to remove any trailing spaces

_EMAIL_RE = re.compile(r"[\w\.-]+@[\w\.-]+\.\w+", re.I)
_PHONE_RE = re.compile(r"(?:\+62|0)[\d\-\s]{7,20}", re.I)

def _is_email(tok: str) -> bool:
    return bool(_EMAIL_RE.fullmatch(tok.strip()))

def _is_phone(tok: str) -> bool:
    return bool(_PHONE_RE.fullmatch(tok.strip()))

def _find_date_range_in_texts(texts: List[str]) -> tuple[Optional[str], Optional[str]]:
    """
    Find date range by looking for specific lines in texts array containing "berlaku sejak tanggal".
    Handles both "hingga" and "sampai dengan" variations, including concatenated dates and trailing text.
    """
    print("ðŸ” DEBUG: _find_date_range_in_texts called")
    print(f"ðŸ” DEBUG: Total texts to search: {len(texts)}")
    
    # Enhanced patterns that handle all possible OCR variations and formatting issues
    date_range_patterns = [
        # Pattern 1: Standard spaced format with "berlaku sejak tanggal"
        r"berlaku\s+sejak\s+tanggal\s+(\d{1,2}\s+\w+\s+\d{4})\s+(?:hingga|sampai\s+dengan)\s+(\d{1,2}\s+\w+\s+\d{4})",
        
        # Pattern 2: Standard format with concatenated end date
        r"berlaku\s+sejak\s+tanggal\s+(\d{1,2}\s+\w+\s+\d{4})\s+(?:hingga|sampai\s+dengan)\s*(\d{1,2}\w+\d{4})",
        
        # Pattern 3: CRITICAL - Concatenated "sejaktanggal" format (like the penerbangan sample)
        r"berlaku\s+sejaktanggal\s+(\d{1,2}\w+\d{4})\s+(?:hingga|sampai\s+dengan)\s*(\d{1,2}\w+\d{4})",
        
        # Pattern 4: Mixed concatenated start, spaced end
        r"berlaku\s+sejaktanggal\s+(\d{1,2}\w+\d{4})\s+(?:hingga|sampai\s+dengan)\s+(\d{1,2}\s+\w+\s+\d{4})",
        
        # Pattern 5: Flexible "berlaku sejak" without "tanggal"
        r"berlaku\s+sejak\s+(\d{1,2}\s+\w+\s+\d{4})\s+(?:hingga|sampai\s+dengan)\s*(\d{1,2}\w+\d{4})",
        
        # Pattern 6: Very flexible fallback - any "sejak" pattern
        r"sejak\s*tanggal?\s*(\d{1,2}[\w\s]+\d{4})\s+(?:hingga|sampai\s+dengan)\s*(\d{1,2}[\w\s]*\d{4})",
        
        # Pattern 7: OCR concatenation variations
        r"berlaku.*?(\d{1,2}\w+\d{4}).*?(?:hingga|sampai.*?dengan).*?(\d{1,2}\w+\d{4})",
        
        # Pattern 8: Very permissive date range finder
        r"(\d{1,2}[\w\s]+\d{4})\s+(?:hingga|sampai\s+dengan)\s*(\d{1,2}[\w\s]*\d{4})"
    ]
    
    # First, look for any lines containing the search phrase
    matching_lines = []
    for i, text_line in enumerate(texts):
        # Enhanced search terms to catch all possible variations
        search_terms = [
            "berlaku sejak tanggal",
            "berlaku sejaktanggal",  # concatenated version
            "berlaku sejak",
            "sejak tanggal", 
            "sejaktanggal"
        ]
        
        if any(term in text_line.lower() for term in search_terms):
            matching_lines.append((i, text_line))
            print(f"ðŸ” DEBUG: Found matching line at index {i}: '{text_line}'")
    
    if not matching_lines:
        print("ðŸ” DEBUG: No lines found containing 'berlaku sejak tanggal'")
        return None, None
    
    for line_idx, text_line in matching_lines:
        print(f"ðŸ” DEBUG: Processing line {line_idx}: '{text_line}'")
        
        # Try all patterns on this line
        for pattern_idx, pattern in enumerate(date_range_patterns):
            print(f"ðŸ” DEBUG: Trying pattern {pattern_idx + 1}: {pattern}")
            match = re.search(pattern, text_line, flags=re.I)
            if match:
                print(f"âœ… DEBUG: Pattern {pattern_idx + 1} MATCHED!")
                start_str = match.group(1).strip()
                end_str = match.group(2).strip()
                print(f"ðŸ” DEBUG: Raw captures - start: '{start_str}', end: '{end_str}'")
                
                # Clean up captured groups - normalize spaces
                start_str = re.sub(r'\s+', ' ', start_str)
                end_str = re.sub(r'\s+', ' ', end_str)
                print(f"ðŸ” DEBUG: Cleaned captures - start: '{start_str}', end: '{end_str}'")
                
                # Additional cleanup for concatenated dates
                # Handle cases like "23Februari2027" -> ensure proper parsing
                if not ' ' in end_str and len(end_str) > 8:  # Likely concatenated format
                    print(f"ðŸ” DEBUG: Detected concatenated end date: '{end_str}'")
                    # Try to add spaces for better parsing: "23Februari2027" -> "23 Februari 2027"
                    concatenated_match = re.match(r'(\d{1,2})([A-Za-z]+)(\d{4})', end_str)
                    if concatenated_match:
                        day, month, year = concatenated_match.groups()
                        end_str_spaced = f"{day} {month} {year}"
                        print(f"ðŸ” DEBUG: Trying spaced version: '{end_str_spaced}'")
                        # Try parsing the spaced version first
                        end_date_spaced = _parse_date_id(end_str_spaced)
                        if end_date_spaced:
                            print(f"âœ… DEBUG: Spaced version parsed successfully: {end_date_spaced}")
                            end_str = end_str_spaced
                        else:
                            print(f"âŒ DEBUG: Spaced version parsing failed")
                
                # Parse both dates
                print(f"ðŸ” DEBUG: Attempting to parse dates...")
                start_date = _parse_date_id(start_str)
                end_date = _parse_date_id(end_str)
                print(f"ðŸ” DEBUG: Parse results - start: {start_date}, end: {end_date}")
                
                # Return if both dates are valid and different
                if start_date and end_date and start_date != end_date:
                    print(f"âœ… DEBUG: SUCCESS! Returning dates: start={start_date}, end={end_date}")
                    return start_date, end_date
                
                # If parsing failed but we had a match, try parsing original concatenated format
                if not end_date and not ' ' in match.group(2).strip():
                    print(f"ðŸ” DEBUG: Retrying with original concatenated format: '{match.group(2).strip()}'")
                    end_date = _parse_date_id(match.group(2).strip())
                    print(f"ðŸ” DEBUG: Retry parse result: {end_date}")
                    if start_date and end_date and start_date != end_date:
                        print(f"âœ… DEBUG: SUCCESS on retry! Returning dates: start={start_date}, end={end_date}")
                        return start_date, end_date
                
                print(f"âŒ DEBUG: Pattern {pattern_idx + 1} matched but date parsing failed")
            else:
                print(f"âŒ DEBUG: Pattern {pattern_idx + 1} did not match")
    
    print("âŒ DEBUG: No successful matches found, returning None, None")
    return None, None

def _extract_jangka_waktu(texts: List[str]) -> tuple[Optional[str], Optional[str]]:
    """
    Enhanced robust extraction untuk jangka waktu kontrak.
    Prioritizes direct extraction from individual text lines containing date ranges.
    """
    # Strategy 1: Direct extraction from texts array - look for "berlaku sejak tanggal" lines
    start_date, end_date = _find_date_range_in_texts(texts)
    if start_date and end_date:
        return start_date, end_date
    
    # Strategy 2: Look in JANGKA WAKTU section with focused approach
    jangka_waktu_patterns = ["6.JANGKA WAKTU", "6. JANGKA WAKTU", "6.JANGKAWAKTU", "JANGKA WAKTU"]
    jangka_idx = _find_near_label(texts, jangka_waktu_patterns)
    
    if jangka_idx is not None:
        # Get section texts and try direct extraction first
        section_texts = texts[jangka_idx:min(jangka_idx + 15, len(texts))]
        start_date, end_date = _find_date_range_in_texts(section_texts)
        if start_date and end_date:
            return start_date, end_date
        
        # Fallback: Look for date patterns in section
        for text_line in section_texts:
            # Simple date range patterns within section
            date_patterns = [
                r"(\d{1,2}\s+\w+\s+\d{4})\s+(?:hingga|sampai\s+dengan)\s+(\d{1,2}[\s\w]*\d{4})",
                r"(\d{1,2}\w+\d{4})\s+(?:hingga|sampai\s+dengan)\s+(\d{1,2}\w+\d{4})"
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, text_line, flags=re.I)
                if match:
                    start_str = match.group(1).strip()
                    end_str = match.group(2).strip()
                    
                    start = _parse_date_id(start_str)
                    end = _parse_date_id(end_str)
                    if start and end and start != end:
                        return start, end
    
    # Strategy 3: Enhanced proximity search for contract date ranges
    date_tokens = []
    seen_dates = set()
    
    # Collect more date candidates for better selection
    for i, tok in enumerate(texts):
        d = _parse_date_id(tok)
        if d and d not in seen_dates:
            date_tokens.append((i, d, tok))  # Include original text for context
            seen_dates.add(d)
        if len(date_tokens) >= 10:  # Get more candidates
            break
    
    print(f"ðŸ” DEBUG: Found {len(date_tokens)} unique dates: {[(i, d) for i, d, _ in date_tokens]}")
    
    if len(date_tokens) >= 2:
        # Smart date pair selection algorithm
        best_pair = None
        best_score = -1
        
        for i in range(len(date_tokens)):
            for j in range(i + 1, len(date_tokens)):
                idx1, date1, text1 = date_tokens[i]
                idx2, date2, text2 = date_tokens[j]
                
                # Ensure chronological order
                start_date, end_date = (date1, date2) if date1 < date2 else (date2, date1)
                start_idx, end_idx = (idx1, idx2) if date1 < date2 else (idx2, idx1)
                
                # Calculate date range (in days)
                from datetime import datetime
                try:
                    start_dt = datetime.strptime(start_date, "%Y-%m-%d")
                    end_dt = datetime.strptime(end_date, "%Y-%m-%d")
                    days_diff = (end_dt - start_dt).days
                except:
                    continue
                
                # Scoring criteria for contract date ranges
                score = 0
                
                # 1. Prefer reasonable contract durations (30 days to 5 years)
                if 30 <= days_diff <= 1825:  # 30 days to 5 years
                    score += 100
                elif 1 <= days_diff <= 30:  # Very short contracts
                    score += 20
                else:
                    score -= 50  # Too short or too long
                
                # 2. Prefer dates that are closer together in the text (contract dates are usually mentioned together)
                text_distance = abs(start_idx - end_idx)
                if text_distance <= 5:
                    score += 50
                elif text_distance <= 10:
                    score += 20
                else:
                    score -= 10
                
                # 3. Penalize obvious signature dates (single day differences or dates at end of document)
                if days_diff <= 2:  # Likely signature dates
                    score -= 100
                
                # 4. Prefer dates from earlier in jangka waktu section
                if start_idx <= 15:  # Earlier in the section
                    score += 30
                
                print(f"ðŸ” DEBUG: Date pair ({start_date}, {end_date}) - days: {days_diff}, text_distance: {text_distance}, score: {score}")
                
                if score > best_score:
                    best_score = score
                    best_pair = (start_date, end_date)
        
        if best_pair and best_score > 0:
            print(f"âœ… DEBUG: Selected best date pair with score {best_score}: {best_pair}")
            return best_pair
        
        # Fallback: if no good pair found, use chronological order but skip obvious signature dates
        dates_only = [date for _, date, _ in date_tokens]
        dates_only.sort()
        
        # Try to find a pair that's not just 1-2 days apart (signature dates)
        for i in range(len(dates_only) - 1):
            for j in range(i + 1, len(dates_only)):
                try:
                    start_dt = datetime.strptime(dates_only[i], "%Y-%m-%d")
                    end_dt = datetime.strptime(dates_only[j], "%Y-%m-%d")
                    days_diff = (end_dt - start_dt).days
                    if days_diff >= 30:  # At least 30 days for a reasonable contract
                        print(f"âœ… DEBUG: Fallback selection: {dates_only[i]}, {dates_only[j]} ({days_diff} days)")
                        return dates_only[i], dates_only[j]
                except:
                    continue
        
        # Last resort: first two different dates
        if len(dates_only) >= 2 and dates_only[0] != dates_only[1]:
            print(f"âš ï¸  DEBUG: Last resort selection: {dates_only[0]}, {dates_only[1]}")
            return dates_only[0], dates_only[1]
    
    return None, None

def _extract_contact_blocks(texts: List[str]) -> tuple[Dict[str, str], Dict[str, str]]:
    """
    Enhanced robust extraction untuk contact blocks:
    - Blok pertama: TELKOM
    - Blok kedua: PELANGGAN
    Menggunakan fuzzy matching dan multiple strategies.
    """
    # Strategy 1: Cari anchor "7.KONTAK PERSON" dengan fuzzy matching
    kontak_patterns = ["7.KONTAK PERSON", "7. KONTAK PERSON", "7.KONTAKPERSON", "KONTAK PERSON", "7KONTAK PERSON"]
    start = _find_near_label(texts, kontak_patterns)
    
    if start is None:
        # Strategy 2: Cari berdasarkan context pattern
        for i, text in enumerate(texts):
            if "kontak" in text.lower() and "person" in text.lower():
                start = i
                break
    
    if start is None:
        return {}, {}

    # Ambil subarray wajar (hingga sebelum tanda tangan)
    sub = texts[start : min(start + 120, len(texts))]

    # Strategy 1: Cari token "TELKOM" dengan fuzzy matching
    telkom_patterns = ["TELKOM", "PT TELKOM", "PT. TELKOM"]
    telkom_anchor = _find_near_label(sub, telkom_patterns, 0)
    
    if telkom_anchor is None:
        # Strategy 2: Cari berdasarkan context pattern
        for i, text in enumerate(sub):
            if "telkom" in text.lower():
                telkom_anchor = i
                break
        if telkom_anchor is None:
            telkom_anchor = 0  # fallback ke awal sub
    
    telkom_tokens = sub[telkom_anchor : ]

    # Enhanced robust contact reading dengan fuzzy matching
    def read_contact(seq: List[str]) -> tuple[Dict[str, str], int]:
        fields = {"nama": None, "jabatan": None, "telepon": None, "email": None}
        field_patterns = {
            "nama": ["Nama", "nama", "NAMA", "Name"],
            "jabatan": ["Jabatan", "jabatan", "JABATAN", "Posisi", "Position"],
            "telepon": ["Telepon", "telepon", "TELEPON", "Tlp", "TLP", "Phone", "No. Tlp"],
            "email": ["Email", "email", "EMAIL", "E-mail", "e-mail", "E-Mail"]
        }
        
        i = 0
        last_label = None
        hits = 0
        
        while i < len(seq):
            tok_raw = seq[i].strip()
            
            # Strategy 1: Exact label matching (existing logic)
            tok_norm = _norm_label(tok_raw)
            matched_field = None
            for field, patterns in field_patterns.items():
                if any(_norm_label(pattern) == tok_norm for pattern in patterns):
                    matched_field = field
                    break
            
            if matched_field:
                # Heuristik switch: jika muncul "Nama" baru DAN sudah ada minimal 2 field â†’ akhir blok
                if matched_field == "nama" and sum(1 for v in fields.values() if v) >= 2:
                    break
                last_label = matched_field
                i += 1
                continue
            
            # Strategy 2: Fuzzy label matching jika exact gagal
            if not matched_field:
                for field, patterns in field_patterns.items():
                    for pattern in patterns:
                        if _calculate_text_similarity(tok_raw, pattern) >= 0.7:
                            matched_field = field
                            break
                    if matched_field:
                        break
                
                if matched_field:
                    last_label = matched_field
                    i += 1
                    continue
            
            # Process value jika ada active label
            if last_label:
                val = tok_raw.strip()
                
                # Enhanced validation dengan OCR error tolerance
                if last_label == "email":
                    # Cek current token dan next token untuk email
                    email_candidates = [val]
                    if i + 1 < len(seq):
                        email_candidates.append(seq[i + 1].strip())
                    
                    valid_email = None
                    for email_candidate in email_candidates:
                        if _is_email(email_candidate):
                            valid_email = email_candidate
                            if email_candidate != val:
                                i += 1  # Skip next token jika itu yang dipake
                            break
                    
                    if valid_email:
                        val = valid_email
                    else:
                        # Coba bersihkan OCR errors umum dalam email
                        cleaned_val = val.replace(' ', '').replace('0', 'o')  # Angka nol jadi huruf o
                        if _is_email(cleaned_val):
                            val = cleaned_val
                        else:
                            last_label = None
                            i += 1
                            continue
                
                elif last_label == "telepon":
                    # Similar logic untuk telepon
                    phone_candidates = [val]
                    if i + 1 < len(seq):
                        phone_candidates.append(seq[i + 1].strip())
                    
                    valid_phone = None
                    for phone_candidate in phone_candidates:
                        if _is_phone(phone_candidate):
                            valid_phone = phone_candidate
                            if phone_candidate != val:
                                i += 1
                            break
                    
                    if valid_phone:
                        val = valid_phone
                    else:
                        # Bersihkan spasi berlebih untuk telepon
                        cleaned_val = re.sub(r'\s+', '', val)
                        if _is_phone(cleaned_val):
                            val = cleaned_val
                        else:
                            last_label = None
                            i += 1
                            continue
                
                # Assign jika field belum terisi dan value valid
                if fields.get(last_label) is None and val and val not in ['', '-', 'N/A']:
                    fields[last_label] = val
                    hits += 1
                
                last_label = None
                i += 1
                
                # Stop heuristik: jika minimal nama + jabatan sudah ketemu
                if hits >= 2 and fields.get("nama") and fields.get("jabatan"):
                    # Lanjut sedikit untuk email/telepon
                    if hits >= 4:
                        break
                continue

            # Heuristik stop patterns
            if any(stop_phrase in tok_raw.lower() for stop_phrase in ["wajib diisi", "tanda tangan", "ttd"]):
                i += 1
                break
            
            i += 1

        return fields, i

    telkom_fields, consumed = read_contact(telkom_tokens)

    # PELANGGAN mulai setelah tokens yang telah dibaca
    pelanggan_tokens = telkom_tokens[consumed:]
    pelanggan_fields, _ = read_contact(pelanggan_tokens)

    # Bersihkan nilai kosong
    telkom_fields = {k: v for k, v in telkom_fields.items() if v}
    pelanggan_fields = {k: v for k, v in pelanggan_fields.items() if v}
    return telkom_fields, pelanggan_fields

# -------------------- Payment Method Specific Page 2 Handling --------------------
def _extract_page2_payment_specific(texts: List[str], payment_method: str) -> Dict[str, Any]:
    """
    Extract additional page 2 information specific to payment method.
    """
    additional_info = {}
    
    if payment_method == "termin":
        # Termin contracts may have additional payment schedule details on page 2
        full_text = " ".join(texts).lower()
        if "jadwal pembayaran" in full_text or "schedule" in full_text:
            additional_info["has_payment_schedule"] = True
            
        # Look for payment coordinator contact
        coordinator_patterns = ["koordinator pembayaran", "payment coordinator", "pic pembayaran"]
        for pattern in coordinator_patterns:
            idx = _find_containing(texts, pattern)
            if idx is not None:
                additional_info["payment_coordinator_context"] = texts[idx]
    
    elif payment_method == "recurring":
        # Recurring may have billing cycle information
        full_text = " ".join(texts).lower()
        if "siklus tagihan" in full_text or "billing cycle" in full_text:
            additional_info["has_billing_cycle"] = True
            
        # Look for automated payment setup
        auto_patterns = ["auto debit", "otomatis", "autopay"]
        for pattern in auto_patterns:
            if pattern in full_text:
                additional_info["auto_payment_mentioned"] = True
    
    elif payment_method == "one_time_charge":
        # One-time may have specific payment deadline information
        full_text = " ".join(texts).lower()
        if "batas waktu pembayaran" in full_text or "payment deadline" in full_text:
            additional_info["has_payment_deadline"] = True
    
    return additional_info

# -------------------- Page 2 Merge --------------------
def merge_with_page2(existing: TelkomContractData, ocr_json_page2: Any) -> TelkomContractData:
    texts = _texts_from_ocr(ocr_json_page2)

    # Get payment method untuk context-aware extraction
    payment_method = existing.tata_cara_pembayaran.method_type if existing.tata_cara_pembayaran else "unknown"
    
    # Payment method specific extraction
    payment_specific_info = _extract_page2_payment_specific(texts, payment_method)

    # 1) Enhanced jangka waktu extraction
    start_date, end_date = _extract_jangka_waktu(texts)
    if existing.jangka_waktu is None:
        existing.jangka_waktu = JangkaWaktu()
    if start_date:
        existing.jangka_waktu.mulai = existing.jangka_waktu.mulai or start_date
    if end_date:
        existing.jangka_waktu.akhir = existing.jangka_waktu.akhir or end_date

    # 2) Enhanced kontak person extraction (TELKOM & PELANGGAN)
    telkom, pelanggan = _extract_contact_blocks(texts)

    # Isi TELKOM
    if any(telkom.values()):
        existing.kontak_person_telkom = KontakPersonTelkom(
            nama=telkom.get("nama"),
            jabatan=telkom.get("jabatan"),
            email=telkom.get("email"),
            telepon=telkom.get("telepon"),
        )

    # Isi PELANGGAN
    if existing.informasi_pelanggan is None:
        existing.informasi_pelanggan = InformasiPelanggan()
    if any(pelanggan.values()):
        existing.informasi_pelanggan.kontak_person = KontakPersonPelanggan(
            nama=pelanggan.get("nama"),
            jabatan=pelanggan.get("jabatan"),
            email=pelanggan.get("email"),
            telepon=pelanggan.get("telepon"),
        )

    return existing


# -------------------- Convenience I/O --------------------
def extract_page1_file(input_json_path: str) -> Dict[str, Any]:
    """Baca file JSON OCR page 1 dan kembalikan dict hasil model_dump()."""
    with open(input_json_path, "r", encoding="utf-8") as f:
        ocr = json.load(f)
    data = extract_from_page1_one_time(ocr)
    # mode="json" memastikan field datetime / non-JSON-native sudah di-serialize (ISO string)
    return data.model_dump(mode="json")

def merge_page2_file(existing_json_path: str, page2_json_path: str, output_json_path: Optional[str] = None) -> Dict[str, Any]:
    """
    Baca hasil existing (file JSON TelkomContractData) + OCR page 2,
    lakukan merge, lalu simpan (opsional) & kembalikan dict.
    """
    with open(existing_json_path, "r", encoding="utf-8") as f:
        existing_dict = json.load(f)
    # Rekonstruksi model dari dict (jika perlu)
    existing = TelkomContractData(**existing_dict)

    with open(page2_json_path, "r", encoding="utf-8") as f:
        ocr2 = json.load(f)

    merged = merge_with_page2(existing, ocr2)
    # Serialize to JSON-friendly structure
    result = merged.model_dump(mode="json")

    if output_json_path:
        with open(output_json_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

    return result


# -------------------- CLI mini --------------------
if __name__ == "__main__":
    import argparse, sys, os
    ap = argparse.ArgumentParser(description="Ekstraksi Telkom Contract (Page 1 one-time + merge Page 2)")
    sub = ap.add_subparsers(dest="cmd", required=True)

    p1 = sub.add_parser("page1", help="Ekstrak dari page 1 (one-time charge)")
    p1.add_argument("--in", dest="inp", required=True, help="Path ke OCR JSON page 1")
    p1.add_argument("--out", dest="out", help="Path simpan hasil TelkomContractData (JSON)")

    m2 = sub.add_parser("merge2", help="Merge hasil page1 dengan OCR page 2")
    m2.add_argument("--existing", required=True, help="File JSON hasil page1 (TelkomContractData)")
    m2.add_argument("--page2", required=True, help="Path ke OCR JSON page 2")
    m2.add_argument("--out", required=True, help="Path simpan hasil merged JSON")

    args = ap.parse_args()

    if args.cmd == "page1":
        res = extract_page1_file(args.inp)
        if args.out:
            with open(args.out, "w", encoding="utf-8") as f:
                json.dump(res, f, ensure_ascii=False, indent=2)
        else:
            json.dump(res, sys.stdout, ensure_ascii=False, indent=2)
    elif args.cmd == "merge2":
        res = merge_page2_file(args.existing, args.page2, args.out)
        # file sudah disimpan; tampilkan ringkas ke stdout
        print(f"Saved merged result to: {args.out}")
